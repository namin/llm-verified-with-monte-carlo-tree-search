nohup: ignoring input
/home/sraja/mambaforge/envs/llm-verified/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/home/sraja/mambaforge/envs/llm-verified/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:05<00:31,  5.23s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:10<00:27,  5.42s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:22,  5.70s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:22<00:17,  5.81s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:29<00:11,  5.99s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:35<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:40<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:40<00:00,  5.73s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
