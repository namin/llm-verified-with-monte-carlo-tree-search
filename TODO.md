# TODOs

- Next steps:
  - [x] Have a flag to support vanilla tokenization, and ensure everything still works when the flag is on.
  - [ ] Support finer granularity in PPO, since tokenizer needs to be vanilla?
  - [ ] Refactor poor man's flags to support running PPO with multiple examples, in a curriculum.
    - [ ] Or enable loading PPO from PPO.
    - [ ] Just enable multiple examples? Avoids reloading the model.
    - [x] Support for iterations.
  - [ ] Challenge: verifier feedback should enable running the prompts without hints.
    - [x] Works for factorial `function method` issue.
  - [ ] Try running with completion depth always 1, and minimally supporting the score of `None`.
- [x] Support verifiers:
  - [x] Dafny
  - [x] Coq
    - [ ] The current system struggles with Coq proofs.
  - [x] Lean
    - [ ] Maybe support custom stopping criteria for '\n\n'?
  - [ ] Idris?
  - [ ] The granularity is too coarse for proving in Lean and Dafny.
        In Coq, we can use . as a separator, but in Dafny and Lean we effectively separate at method boundaries.
- [x] Make Dafny and Coq usable locally, and that be the default.
  - [x] Add a sanity check for when a verifier is not available.
  - [x] Still provide option to use livecode.ch for occasional use.
- [ ] Support other LLM infrastructures in addition to Hugging Face:
  - [ ] [Ollama](https://ollama.ai)
  - [x] OpenAI GPT-4
  - [ ] Have an option to replicate the non-custom tokenizer with open models
- [ ] Support test cases.
- [ ] Design a steerable interaction to give human or tool feedback to the LLM.
  - [x] Confirm completion at each step, and add a comment to steer.
  - [ ] Edit entire text, e.g. to add imports.
  - [ ] Give the verifier feedback.
    - [x] Rudimentary display of goal context in case of error:
      - [x] Coq works better than Dafny.
- [ ] Design higher level steerable schemes:
  - [ ] Synthesize Coq proofs.
- [ ] The LLM can get stuck repeating the same completion over and over again, if it doesn't have a diverse set of outputs.
- [x] Design a reinforcement learning scheme, whereas the LLM learns from trial.
  - [ ] Evaluate whether the model after PPO suffers degradation for some tasks, even unrelated.
  - [ ] Force the PPO solution to converge to an optimal known one, using it entirely for training rather than discovery.
- [x] Get wandb to work.
- [ ] Maybe add a per language blacklist, so that we can rule out uses of `Admitted` in Coq.
- [x] Refactor the shared arguments between llm.py and ppo.py.
- [ ] Support progressive proofs: proving with weaker consequences, and strengthening them after succeeding in steps.
- [ ] Improve the syntax guidance.
- [ ] Glitch: sometimes, the code is not in triple quotes, causing long running completion generations.
- [ ] Why Coq is much harder than Dafny:
  - [ ] Dafny has more automation.
  - [ ] Coq can have idempotent tactics (like `simpl`) which can be repeated uselessly indefinitely.
